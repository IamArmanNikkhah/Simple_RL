{"cells":[{"cell_type":"code","source":"import gym\nfrom collections import namedtuple\nimport numpy as np\nfrom tensorboardX import SummaryWriter\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nHIDDEN_SIZE = 128\nBATCH_SIZE = 16\nPERCENTILE = 70\n\n\nclass Net(nn.Module):\n    def __init__(self, obs_size, n_actions):\n        super(Net, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(obs_size, 3 * obs_size),\n            nn.ReLU(),\n            nn.Linear(3 * obs_size, 3 * obs_size),\n            nn.ReLU(),\n            nn.Linear(3 * obs_size, 3 * obs_size),\n            nn.ReLU(),\n            nn.Linear(3 * obs_size, 3 * obs_size),\n            nn.ReLU(),\n            nn.Linear(3 * obs_size, n_actions)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nEpisode = namedtuple('Episode', field_names=['reward', 'steps'])\nEpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n\n\ndef iterate_batches(env, net, batch_size):\n    batch = []\n    episode_reward = 0.0\n    episode_steps = []\n    obs = env.reset()\n    sm = nn.Softmax(dim=1)\n    while True:\n        obs_v = torch.FloatTensor([obs])\n        act_probs_v = sm(net(obs_v))\n        act_probs = act_probs_v.data.numpy()[0]\n        action = np.random.choice(len(act_probs), p=act_probs)\n        next_obs, reward, is_done, _ = env.step(action)\n        episode_reward += reward\n        episode_steps.append(EpisodeStep(observation=obs, action=action))\n        if is_done:\n            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n            episode_reward = 0.0\n            episode_steps = []\n            next_obs = env.reset()\n            if len(batch) == batch_size:\n                yield batch\n                batch = []\n        obs = next_obs\n\n\ndef filter_batch(batch, percentile):\n    rewards = list(map(lambda s: s.reward, batch))\n    reward_bound = np.percentile(rewards, percentile)\n    reward_mean = float(np.mean(rewards))\n\n    train_obs = []\n    train_act = []\n    for example in batch:\n        if example.reward < reward_bound:\n            continue\n        train_obs.extend(map(lambda step: step.observation, example.steps))\n        train_act.extend(map(lambda step: step.action, example.steps))\n\n    train_obs_v = torch.FloatTensor(train_obs)\n    train_act_v = torch.LongTensor(train_act)\n    return train_obs_v, train_act_v, reward_bound, reward_mean\n\nif __name__ == \"__main__\":\n    env = gym.make(\"CartPole-v0\")\n    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n    obs_size = env.observation_space.shape[0]\n    n_actions = env.action_space.n\n\n    net = Net(obs_size, n_actions)\n    objective = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(params=net.parameters(), lr=0.003)\n    writer = SummaryWriter(comment=\"-cartpole\")\n\n    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n        optimizer.zero_grad()\n        action_scores_v = net(obs_v)\n        loss_v = objective(action_scores_v, acts_v)\n        loss_v.backward()\n        optimizer.step()\n        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n            iter_no, loss_v.item(), reward_m, reward_b))\n        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n        if reward_m > 199:\n            print(\"Solved!\")\n            break\n    writer.close()","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}